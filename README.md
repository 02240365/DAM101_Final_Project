# Dzongkha Digit Classification Using Deep Learning Architectures

A comprehensive deep learning solution for recognizing traditional Dzongkha numerals (à¼ -à¼©) using computer vision techniques. This project implements and compares three distinct neural network architectures: Custom CNN, ResNet, and VGG16.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Data Preparation](#data-preparation)
- [Model Architectures](#model-architectures)
- [Performance Metrics](#performance-metrics)
- [Deployment](#deployment)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [License](#license)
- [References](#references)

## ğŸ¯ Project Overview

### Objectives

This project addresses the critical challenge of digitizing Bhutanese cultural heritage by developing an automated recognition system for traditional Dzongkha numerals. The main objectives include:

- **Cultural Preservation**: Enable digital archival of historical documents containing Dzongkha numerals
- **Educational Applications**: Support learning tools for Dzongkha language education
- **Administrative Efficiency**: Automate data entry for government and institutional documents
- **Accessibility**: Improve digital accessibility for Dzongkha speakers and learners

### Problem Statement

Traditional OCR systems primarily focus on Latin scripts and Arabic numerals, with limited support for Dzongkha numerals (à¼ , à¼¡, à¼¢, à¼£, à¼¤, à¼¥, à¼¦, à¼§, à¼¨, à¼©). This project fills that gap by providing a specialized deep learning solution.

## âœ¨ Features

- **Multi-Architecture Comparison**: Implementation of CNN, ResNet, and VGG16 architectures
- **Advanced Preprocessing**: Automated grid analysis and digit extraction from 17Ã—10 grid images
- **High Accuracy**: Best model achieves 95.2% accuracy on test dataset
- **Production Ready**: Deployed on HuggingFace Spaces with user-friendly interface
- **Comprehensive Evaluation**: Detailed performance metrics and comparative analysis
- **Mixed Precision Training**: Optimized training with 40% speed improvement

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (optional, for faster training)
- 8GB+ RAM recommended

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/dzongkha-digit-classification.git
   cd dzongkha-digit-classification
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\\Scripts\\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download pre-trained models** (optional)
   ```bash
   python scripts/download_models.py
   ```

## ğŸ“– Usage

### Quick Start

1. **Single Image Prediction**
   ```python
   from src.models.predictor import DzongkhaDigitPredictor
   
   # Initialize predictor with best model (ResNet)
   predictor = DzongkhaDigitPredictor(model_path='models/resnet_best.pth')
   
   # Predict single digit
   result = predictor.predict('path/to/digit_image.png')
   print(f"Predicted digit: {result['digit']}")
   print(f"Confidence: {result['confidence']:.2f}")
   ```

2. **Batch Processing**
   ```python
   from src.data.batch_processor import BatchProcessor
   
   processor = BatchProcessor(model_path='models/resnet_best.pth')
   results = processor.process_directory('data/test_images/')
   ```

3. **Web Interface**
   ```bash
   python app.py
   # Open http://localhost:5000 in your browser
   ```

### Expected Outputs

**Single Prediction Output:**
```json
{
  "predicted_digit": "à¼£",
  "confidence": 0.96,
  "all_probabilities": {
    "à¼ ": 0.01, "à¼¡": 0.02, "à¼¢": 0.01, "à¼£": 0.96,
    "à¼¤": 0.00, "à¼¥": 0.00, "à¼¦": 0.00, "à¼§": 0.00,
    "à¼¨": 0.00, "à¼©": 0.00
  }
}
```

**Training Output:**
```
Epoch 1/30: Train Loss: 2.1234, Train Acc: 45.2%, Val Loss: 1.8765, Val Acc: 52.1%
Epoch 5/30: Train Loss: 0.8234, Train Acc: 78.5%, Val Loss: 0.7123, Val Acc: 81.2%
...
Best Model: ResNet at Epoch 27 with 95.2% validation accuracy
```

## ğŸ“Š Data Preparation

### Dataset Structure

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ grid_images/          # Original 17Ã—10 grid images
â”‚   â””â”€â”€ metadata.json         # Image metadata
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ train/                # Training samples (70%)
â”‚   â”œâ”€â”€ val/                  # Validation samples (15%)
â”‚   â””â”€â”€ test/                 # Test samples (15%)
â””â”€â”€ extracted/
    â””â”€â”€ digits/               # Individual digit images (28Ã—28)
```

### Data Preparation Guidelines

1. **Grid Image Requirements**
   - Format: PNG or JPG
   - Resolution: Minimum 1000Ã—600 pixels
   - Layout: 17 rows Ã— 10 columns of digits
   - Clear digit boundaries with minimal noise

2. **Automated Extraction Process**
   ```bash
   python scripts/extract_digits.py --input data/raw/grid_images/ --output data/extracted/
   ```

3. **Quality Assessment**
   - Automatic quality scoring based on variance, edge content, and contrast
   - Minimum quality threshold: 0.3
   - Manual validation recommended for edge cases

4. **Data Augmentation**
   - Rotation: Â±15 degrees
   - Translation: Â±10%
   - Scaling: 0.9-1.1x
   - Applied during training only

### Dataset Statistics

- **Total Samples**: 3,264 high-quality digits
- **Distribution**: Balanced across all 10 digit classes
- **Quality Distribution**: 45% high, 35% medium, 20% low quality
- **Train/Val/Test Split**: 70%/15%/15%

## ğŸ—ï¸ Model Architectures

### 1. Custom CNN
- **Architecture**: 3 convolutional blocks + 2 fully connected layers
- **Parameters**: 1.2M
- **Features**: Batch normalization, dropout regularization
- **Performance**: 94.8% test accuracy

### 2. Custom ResNet (Best Model)
- **Architecture**: ResNet blocks with skip connections
- **Parameters**: 890K (most efficient)
- **Features**: Adaptive average pooling, residual connections
- **Performance**: 95.2% test accuracy

### 3. Custom VGG16
- **Architecture**: VGG-inspired with multiple 3Ã—3 convolutions
- **Parameters**: 2.1M
- **Features**: Deep feature extraction, adaptive pooling
- **Performance**: 93.6% test accuracy

### Training Configuration

```python
HYPERPARAMS = {
    'ResNet': {
        'learning_rate': 0.001,
        'max_lr': 0.005,
        'weight_decay': 1e-4,
        'dropout_rate': 0.3,
        'batch_size': 128,
        'epochs': 30
    }
}
```

## ğŸ“ˆ Performance Metrics

### Overall Results

| Model  | Test Accuracy | Training Epochs | Parameters | Early Stopped |
|--------|---------------|-----------------|------------|---------------|
| CNN    | 94.8%         | 23/30          | 1.2M       | Yes           |
| ResNet | **95.2%**     | 27/30          | 890K       | Yes           |
| VGG16  | 93.6%         | 30/30          | 2.1M       | No            |

### Per-Class Accuracy (ResNet - Best Model)

| Digit | Dzongkha | Accuracy |
|-------|----------|----------|
| 0     | à¼         | 96.8%    |
| 1     | à¼¡        | 97.1%    |
| 2     | à¼¢        | 95.9%    |
| 3     | à¼£        | 94.7%    |
| 4     | à¼¤        | 93.9%    |
| 5     | à¼¥        | 94.2%    |
| 6     | à¼¦        | 93.8%    |
| 7     | à¼§        | 95.5%    |
| 8     | à¼¨        | 96.1%    |
| 9     | à¼©        | 94.8%    |

### Training Optimizations

- **OneCycleLR Scheduler**: Faster convergence
- **Mixed Precision Training**: 40% speed improvement
- **Early Stopping**: Prevents overfitting (patience=7)
- **Data Augmentation**: Improves generalization

## ğŸš€ Deployment

### HuggingFace Spaces

The model is deployed on HuggingFace Spaces for public access:
- **URL**: [https://huggingface.co/spaces/zsonam/dzongkha_digit_classification](https://huggingface.co/spaces/yourusername/dzongkha-digits)
- **Interface**: Gradio-based web interface
- **Features**: Image upload, webcam capture, drawing canvas


## ğŸ“ Project Structure

```
dzongkha-digit-classification/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â””â”€â”€ grid_extractor.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ cnn.py
â”‚   â”‚   â”œâ”€â”€ resnet.py
â”‚   â”‚   â”œâ”€â”€ vgg16.py
â”‚   â”‚   â””â”€â”€ predictor.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ evaluator.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_digits.py
â”‚   â”œâ”€â”€ train_models.py
â”‚   â””â”€â”€ evaluate_models.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_exploration.ipynb
â”‚   â”œâ”€â”€ model_comparison.ipynb
â”‚   â””â”€â”€ results_analysis.ipynb
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ resnet_best.pth
â”‚   â”œâ”€â”€ cnn_best.pth
â”‚   â””â”€â”€ vgg16_best.pth
â”œâ”€â”€ data/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ report.pdf
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š References

For detailed methodology, results analysis, and technical implementation details, please refer to the comprehensive project report: **[Dzongkha Digit Classification Report](DAM101_Report(02240365).pdf)**

### Key Publications

1. LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*.
2. He, K., et al. (2016). Deep residual learning for image recognition. *CVPR*.
3. Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. *arXiv*.

## ğŸ™ Acknowledgments

- Bhutanese cultural heritage preservation initiatives
- PyTorch and HuggingFace communities
- Google Colab for computational resources
- Contributors to open-source deep learning frameworks

